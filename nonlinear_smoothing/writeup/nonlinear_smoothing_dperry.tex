\documentclass[11pt]{article}
%\usepackage{fullpage,url}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage[letterpaper,top=1in,bottom=1in,left=1in,right=1in,nohead]{geometry}

\setlength{\parindent}{0in}
\setlength{\parskip}{6pt}
\bibliographystyle{plain}

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Unif}{Unif}
\DeclareMathOperator{\shrink}{shrink}

\begin{document}
\thispagestyle{empty}
{\large{\bf CS7640: Advanced Image Processing \hfill Danny Perry}}\\

{\LARGE{\bf Non-linear Image Denoising}}
\vspace{0.2\baselineskip}
\hrule

\section{Introduction}
One of the classic problems in image analysis is image denoising.
We are interested in the specific approaches to image denoising that preserve edges.
A classic approach to edge-preserving non-linear denoising was introduced by \cite{perona1990scale}, where edge preservation was achieved by modeling the problem as an anisotropic diffusion problem.
Later work in \cite{rudin1992nonlinear} introduced the so called "ROF Model" where edge-preserving denoising could be done my minimizing the total variation of the image.
Total variation minimization is based on the principle that noisy images have high total variation, which is the integral of the norm of the gradient.  
By reducing the total variation subject to being a close match to the original image removes unwanted detail (hopefully noise), while preserving important details like edges \cite{wikiTV}.

Here we evaluate three popular algorithms in minimization of total variation:
\begin{enumerate}
\item TV using Chambolle's Algorithm
\item TV using Primal-Dual Algorithm
\item Anisotropic TV using Split-Bregman Iterations
\end{enumerate}

\section{Methods}
Here we briefly describe the specific motivation for each algorithm, how they work in theory, and the specific engineering decisions used in the implementations.
\subsection{Total Variation Minimization}
Here we motivate and describe the method of total variation.
As briefly described above varation can be described like so:
\begin{equation*}
\int |\nabla u| dx dy
\end{equation*}
Where we want to minimize the varation while maintaining some similarity with the original image, we hope to minimize the following constrained problem:
\begin{equation}
\min \int |\nabla u| dx dy \text{ s.t.} \int (u-u_0)^2 dxdy = \sigma^2
\label{tv:isotropic}
\end{equation}
This specific formulation is referred to colloquially as the ROF model.
When originally presented in \cite{rudin1992nonlinear}, a gradient descent type algorithm was suggested to minimize the Euler-Lagrange equations of the problem.
While the results were good, the specific algorithm for solving it was numerically difficult, due to the possibility of $|\nabla u|=0$.
Later work, presented below attempted to improve on that approach.

\subsection{Chambolle's Algorithm}
In \cite{chambolle2004algorithm} a new formulation was presented introducing a so called "dual problem", which addressed the possible zero gradients by changing the formulation into a constrained maximization problem, ie
\begin{align*}
\int |\nabla u| dx xy &= \max_{|p| \le 1} \int p \nabla u dx dy \\
&= \max_{|p| \le 1} \int (\nabla \cdot p) u dx dy
\end{align*}
The second equation is obtained via integration by parts.  
In this form, the problem can be solved in a two-step approach:
\begin{enumerate}
\item
Gradient descent to maximize $p$:
\begin{equation}
p^{k+1} = p^{k} - \Delta t \left[ \nabla( \nabla \cdot p^{k}) + \lambda \nabla u_0 \right]
\label{ch:dual}
\end{equation}
\item Solution to the tv minimization:
\begin{equation}
u = f + \lambda \nabla \cdot \hat{p}
\label{ch:primal}
\end{equation}
\end{enumerate}
In practice this two step approach is repeated until desirable results are reached.

Here the implementation was done entirely in C++ using the ITK framework.
ITK was used primarily for image and vector data structures, I/O, and some infrastructure for parallelization.
Any actual numerics were rewritten for this project, including gradient and divergence operations.
A reader interested in following the code would be directed to primarily read the respective "GenerateData()" and "ThreadedGenerateData()" methods of the .hxx files of the classes described below, as that is where the action takes place - the rest is infrastructure. 
A README.txt file in the code directory describes how to build the executable.

In order to parallelize the algorithm, it was decomposed into a few distinct parallel steps over the image, with a single threaded iteration loop.  
The structure follows directly the algorithm structure:
\begin{enumerate}
\item
ChambolleFilter.h,.hxx - the main loop
\item
ChambolleDualFilter.h,.hxx - the parallel dual solution
\item
ChambollePrimalFilter.h,.hxx - the parallel primal solution \\
DivergenceFilter.h,.hxx - the parallel divergence computation \\
UnitGradientFilter.h,.hxx - the parallel unit gradient computation
\end{enumerate}

As discussed in class, I found it important to "balance" the one-sided derivatives used, so that the result ended up "on the grid" and not in the "gap".
For example, in this solution I use a "right-sided" difference for the gradient and a "left-sided" difference for the divergence.

The implementation was done primarily from in class notes, which some details taken from the original paper.

It's also worth noting that because of the double loop needed to obtain quality results, this algorithm is the slowest of the three algorithms investigated.
Specific results will be discussed below.

\subsection{Primal-Dual Algorithm}
A more efficient solution to the primal and dual problems called the Primal-Dual algorithm was described in \cite{zhu2008efficient}.
Instead of solving the dual problem to completion (\ref{ch:dual}) and then using that solution in the primal (\ref{ch:primal}), the Primal-Dual algorithm solves both problems simultaneously in a sort of combined gradient descent.

In a single main iteration two steps are used to find the solution to the primal and dual equations:
\begin{enumerate}
\item Dual step:
\begin{equation}
p^{k+1} = P_p(p^k + \tau_k \lambda A^T u^k)
\label{pd:dual}
\end{equation}
Where $P_p(z)$ is the projection back onto $p$.
\item Primal step:
\begin{equation}
u^{k+1} = u^k - \theta_k (1/\lambda Ap^{k+1} + u^k-u_0)
\label{pd:primal}
\end{equation}
\end{enumerate}

Again, this was implemented in a parallel fashion using C++ and ITK.
A single thread loop controlled the main iteration, while separate parallel processes compute each step.
Here is the specific break down:
\begin{enumerate}
\item PrimalDualFilter.h,.hxx - the main loop
\item DualFilter.h,.hxx - the parallel dual step
\item PrimalFilter.h,.hxx - the parallel primal step\\
UnitGradientFilter.h,.hxx - parallel unit gradient computation
\end{enumerate}

Again, the "balance" of the derivatives was very important here.
Similar to above the gradient used a "right-sided" difference, while the divergence was computed using a "left-sided" difference.

While I found it necessary to compute the initial $p$ unit gradient field, all other gradient and divergence calculations are done in the dual and primal step filters themselves.

I primarily used the original paper to do the implementation.

This algorithm was much faster than Chambolle's, but specific results are shown below.

\subsection{Split-Bregman Algorithm}
The most recent of those approaches discussed here, is the Split-Bregman approach described in \cite{goldstein2009split}.  
While the two methods discussed above are quite similar, this approach takes a slightly different route.
Also, while the above algorithms minimize isotropic total variation, the algorithm implemented here minimizes an anisotropic total variation.
The same paper \cite{goldstein2009split} describes an isotropic version of Split-Bregman, but is not discussed in this report.

Instead of solving the problem described in \ref{tv:isotropic}, the anisotropic Split-Bregman addresses this similar anisotropic problem:
\begin{equation}
\arg \min_{u}\left[ |du/dx|_1 + |du/dy|_1 + \frac{\mu}{2}||u-u_o||^2_2 \right]
\label{tv:anisotropic}
\end{equation}

The basic algorithm is as follows:
\begin{enumerate}
\item A single Gauss-Seidel (or similar) step to solve the $L_2$ problem.
\begin{equation*}
u^{k+1} = G(u^k,d^k,b^k)
\end{equation*}
\item A shrinkage step
\begin{equation*}
d^{k+1}_x = \shrink(\nabla_x u^{k+1} + b^k_x, 1/\lambda)
\end{equation*}
\begin{equation*}
d^{k+1}_y = \shrink(\nabla_y u^{k+1} + b^k_y, 1/\lambda)
\end{equation*}
\item A Bregman update
\begin{equation*}
b^{k+1}_x = b^k_x + (\nabla_x u^{k+1} - d^{k+1}_x)
\end{equation*}
\begin{equation*}
b^{k+1}_y = b^k_y + (\nabla_y u^{k+1} - d^{k+1}_y)
\end{equation*}

\end{enumerate}

This was also implemented in C++ using ITK.
This algorithm was surprisingly simple to implement.
It's the only algorithm where I simply coded up the algorithm, compiled, and it worked.
This may have been due to it being the last one implemented, but I'd like to think it's somewhat due to the simplicity and robustness of the approach.
It was also surprising how fast it is.  
When I first completed the code I thought something must be wrong, but it just converges very quickly.

To parallelize this code, I broke the algorithm into two distinct parallel steps, the Gauss-Seidel step and a combined shrinkage and Bregman update step, with a single iteration loop:

\begin{enumerate}
\item SplitBregman.h,.hxx - the main loop
\item GaussSeidelFilter.h,.hxx - the parallel Gauss-Seidel step
\item ShrinkBregmanFilter.h,.hxx - the parallel Shrink and Bregman update step
GradientFilter.h,.hxx - a parallel gradient filter
\end{enumerate}

This code was mainly implemented using the original paper \cite{goldstein2009split}, I also got a few details like edge conditions, etc. from Goldsteins implementation available for download at his website \cite{goldsteinImplmentation}.

\section{Results}
\subsection{Chambolle's Algorithm}
\subsection{Primal-Dual Algorithm}
\subsection{Split-Bregman Algorithm}

\section{Discussion}

\section{Conclusion}

\bibliography{nonlinear_smoothing}

\end{document}
